{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Hugging Face Models (L40S Compatible)\n",
    "\n",
    "This notebook will guide you through:\n",
    "1. Setting up your environment for Hugging Face\n",
    "2. Pulling a model from the Hugging Face Hub\n",
    "3. Loading and using the model for inference\n",
    "4. Fine-tuning a model for your specific needs (basic example)\n",
    "5. Saving and sharing your model\n",
    "\n",
    "All examples are optimized to run on a single NVIDIA L40S GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Your Environment\n",
    "\n",
    "First, we need to install the necessary packages. We'll install the exact versions needed to avoid compatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install transformers==4.36.2 \\\n",
    "          datasets==2.15.0 \\\n",
    "          torch==2.1.2 \\\n",
    "          accelerate==0.25.0 \\\n",
    "          evaluate==0.4.1 \\\n",
    "          pillow==10.1.0 \\\n",
    "          huggingface-hub==0.19.4 \\\n",
    "          sentencepiece==0.1.99 \\\n",
    "          protobuf==4.25.1 \\\n",
    "          safetensors==0.4.1 \\\n",
    "          hf_xet  # Adding this to address the warnings\n",
    "\n",
    "# Check the version to make sure installation was successful\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Hugging Face Authentication\n",
    "\n",
    "You'll need to authenticate with Hugging Face to access certain models. Let's set this up properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# IMPORTANT: How to create a token with the right scope\n",
    "print(\"To create a Hugging Face token with the appropriate scope:\")\n",
    "print(\"1. Visit https://huggingface.co/settings/tokens\")\n",
    "print(\"2. Click 'New token'\")\n",
    "print(\"3. Give it a name (e.g., 'Notebook Access')\")\n",
    "print(\"4. For Role, select 'Read' if you only need to download models\")\n",
    "print(\"5. Select 'Write' if you plan to upload your models to the Hub\")\n",
    "print(\"6. Click 'Generate a token'\")\n",
    "print(\"7. Copy the token and paste it below when prompted\")\n",
    "print(\"\\nNote: If the login cell takes too long, you can set the token directly:\")\n",
    "print(\"import os\")\n",
    "print(\"os.environ['HUGGINGFACE_TOKEN'] = 'your_token_here'\")\n",
    "\n",
    "# Optionally set token directly to avoid the interactive prompt\n",
    "# Uncomment the lines below and replace with your token\n",
    "# import os\n",
    "# os.environ['HUGGINGFACE_TOKEN'] = 'your_token_here'  # Replace with your actual token\n",
    "# print(\"Token set via environment variable\")\n",
    "\n",
    "# Or use the interactive login\n",
    "# Comment this out if you used the environment variable approach above\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pulling Models from Hugging Face Hub\n",
    "\n",
    "Let's start with a smaller, efficient model that works well on an L40S GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# First check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# This is a lightweight sentiment analysis model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load the tokenizer and model with explicit truncation settings to avoid warnings\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model size: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Model and Tokenizer\n",
    "\n",
    "Let's explore what the model can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model's configuration\n",
    "print(\"Model configuration:\")\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using the Model for Inference\n",
    "\n",
    "Now let's use our model to analyze some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to get sentiment predictions\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenize the input text with explicit truncation setting\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get predictions from the model\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Convert to probabilities with softmax\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Get the most likely class\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    # Map class index to label\n",
    "    sentiment = \"positive\" if predicted_class == 1 else \"negative\"\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return sentiment, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with some example texts\n",
    "examples = [\n",
    "    \"I love this product! It's amazing and works perfectly.\",\n",
    "    \"The service was terrible and the staff was rude.\",\n",
    "    \"The movie was okay, not great but not terrible either.\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    sentiment, confidence = analyze_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment} (Confidence: {confidence:.4f})\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a Different Model: Text Generation\n",
    "\n",
    "Let's also try a text generation model optimized for L40S GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Using a small GPT-2 model for text generation that fits comfortably on an L40S\n",
    "gen_model_name = \"distilgpt2\"\n",
    "\n",
    "# Create a text generation pipeline with explicit truncation\n",
    "generator = pipeline(\n",
    "    'text-generation', \n",
    "    model=gen_model_name, \n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    "    truncation=True  # Explicitly set truncation\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"In a world where AI and humans work together, \"\n",
    "result = generator(\n",
    "    prompt, \n",
    "    max_length=50, \n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=50256  # Explicitly set pad token to avoid warning\n",
    ")\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning a Model for Your Specific Task\n",
    "\n",
    "Let's see how to fine-tune a model on a small dataset to fit within L40S GPU memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load a small subset of the dataset to fit in memory\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\", split={'train': 'train[:2000]', 'validation': 'validation[:500]'})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine some examples from the dataset\n",
    "for i in range(3):\n",
    "    print(f\"Text: {dataset['train'][i]['text']}\")\n",
    "    print(f\"Label: {dataset['train'][i]['label']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for fine-tuning\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Load a small model for fine-tuning\n",
    "model_name_ft = \"distilbert-base-uncased\"\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(model_name_ft)\n",
    "model_ft = AutoModelForSequenceClassification.from_pretrained(model_name_ft, num_labels=3)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare train and validation datasets\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments - NOTE: fixed previous error with evaluation_strategy\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy to eval_strategy\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Reduced batch size to fit in memory\n",
    "    per_device_eval_batch_size=8,   # Reduced batch size to fit in memory\n",
    "    num_train_epochs=2,             # Reduced epochs for demonstration\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    fp16=True                       # Use mixed precision for faster training\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model_ft,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "# Uncomment to run the training\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving Your Fine-tuned Model Locally\n",
    "\n",
    "After fine-tuning, you can save your model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've trained the model, you can save it\n",
    "# Uncomment these lines after running training\n",
    "\"\"\"\n",
    "# Save the model and tokenizer locally\n",
    "model_ft.save_pretrained(\"./my_finetuned_model\")\n",
    "tokenizer_ft.save_pretrained(\"./my_finetuned_model\")\n",
    "print(\"Model saved to ./my_finetuned_model\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sharing Your Model on Hugging Face Hub (Optional)\n",
    "\n",
    "If you want to share your model with the community, you can push it to Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to upload your model to Hugging Face Hub\n",
    "# Uncomment and modify for your needs\n",
    "\"\"\"\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# Define your model name (must be unique on your account)\n",
    "model_name = \"your-username/tweet-sentiment-model\"\n",
    "\n",
    "# Push the model to the Hub\n",
    "if HfFolder.get_token() is not None:  # Check if logged in\n",
    "    trainer.push_to_hub(model_name, private=True)  # Set private=False to make it public\n",
    "    print(f\"Model pushed to Hugging Face Hub: {model_name}\")\n",
    "else:\n",
    "    print(\"You need to login first using login() function\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using Pre-trained Models for Other Tasks\n",
    "\n",
    "Let's explore some other task-specific models that work well on the L40S GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "Let's use a more efficient NER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a named entity recognition pipeline with a smaller model\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\", \n",
    "    model=\"dslim/bert-base-NER\",  # Smaller model than previous example\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "# Try it out\n",
    "text = \"My name is Sarah and I work at Google in New York City.\"\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "print(\"Named Entities:\")\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']}: {entity['entity']} (Score: {entity['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification\n",
    "\n",
    "Let's use a more memory-efficient image classifier for the L40S GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PIL explicitly if not already installed\n",
    "!pip install pillow\n",
    "\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Download an example image\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Display the image in Jupyter notebook\n",
    "display(image)  # Works better in Jupyter than image.show()\n",
    "\n",
    "# Load a smaller image classification model\n",
    "model_name = \"microsoft/resnet-18\"  # Smaller than ViT, works better on L40S\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "model = model.to(device)  # Move to GPU\n",
    "\n",
    "# Preprocess the image and get predictions\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}  # Move to same device as model\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploying a Model for Production\n",
    "\n",
    "Here's a simple Flask API example for deploying your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for a Flask API - save to a file called app.py\n",
    "\"\"\"\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set device - use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model (do this outside of route to load only once)\n",
    "model_path = \"./my_finetuned_model\"  # Or use a model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model = model.to(device)  # Move model to GPU if available\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    text = data['text']\n",
    "    \n",
    "    # Tokenize and predict\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move to same device as model\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    # For tweet_eval sentiment: 0 = negative, 1 = neutral, 2 = positive\n",
    "    sentiment_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    sentiment = sentiment_map[predicted_class]\n",
    "    \n",
    "    return jsonify({\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': confidence\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "\"\"\"\n",
    "\n",
    "# To run this:\n",
    "# python app.py\n",
    "# Then, make POST requests to http://localhost:5000/predict with JSON data: {\"text\": \"your text here\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Optimizing for L40S GPU\n",
    "\n",
    "Here are some techniques to get the most out of your L40S GPU when working with Hugging Face models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "Quantization can significantly reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using 8-bit quantization\n",
    "\"\"\"\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load model in 8-bit precision\n",
    "model_8bit = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"8-bit model loaded. Memory usage significantly reduced!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checkpointing\n",
    "\n",
    "For training larger models on the L40S, gradient checkpointing trades compute for memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using gradient checkpointing\n",
    "\"\"\"\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "\n",
    "print(\"Gradient checkpointing enabled. This will use less memory during training.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Precision Training\n",
    "\n",
    "Using mixed precision can significantly speed up training on the L40S GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of training arguments with mixed precision\n",
    "\"\"\"\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    fp16_opt_level=\"O1\",  # Optimization level\n",
    "    # ... other arguments\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Selecting the Right Models for L40S\n",
    "\n",
    "Here are some of the most popular and efficient models that work well on a single L40S GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "- distilbert-base-uncased\n",
    "- roberta-base\n",
    "- albert-base-v2\n",
    "\n",
    "### Named Entity Recognition\n",
    "- dslim/bert-base-NER\n",
    "- dbmdz/bert-base-cased-finetuned-conll03-english\n",
    "\n",
    "### Text Generation\n",
    "- distilgpt2\n",
    "- EleutherAI/gpt-neo-125M\n",
    "- bigscience/bloom-560m\n",
    "\n",
    "### Image Classification\n",
    "- microsoft/resnet-18\n",
    "- microsoft/resnet-50\n",
    "- google/vit-base-patch16-224\n",
    "\n",
    "### Translation\n",
    "- Helsinki-NLP/opus-mt-en-fr (or other language pairs)\n",
    "- facebook/mbart-large-50-one-to-many-mmt\n",
    "\n",
    "### Summarization\n",
    "- facebook/bart-base\n",
    "- t5-small\n",
    "- google/pegasus-xsum\n",
    "\n",
    "These models strike a good balance between performance and efficiency, making them suitable for an L40S GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've covered how to work with Hugging Face models on an NVIDIA L40S GPU, including:\n",
    "\n",
    "- Setting up your environment with the right dependencies\n",
    "- Properly authenticating with Hugging Face\n",
    "- Loading and using pre-trained models for various tasks\n",
    "- Fine-tuning models with memory-efficient settings\n",
    "- Optimizing models for inference and deployment\n",
    "- Selecting the right models that work well on a single L40S GPU\n",
    "\n",
    "By following these best practices, you can effectively leverage Hugging Face models even with GPU memory constraints."
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### Next Steps\n",
     "\n",
     "1. Explore the [Model Hub](https://huggingface.co/models) for more L40S-compatible models\n",
     "2. Try quantization techniques for using larger models on limited hardware\n",
     "3. Experiment with parameter-efficient fine-tuning techniques like LoRA or Adapters\n",
     "4. Join the Hugging Face community to share your experiences and learn from others\n",
     "\n",
     "Happy modeling!"
    ]
   }
  ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.8.10"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
