{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Hugging Face Models\n",
    "\n",
    "This notebook will guide you through:\n",
    "1. Setting up your environment for Hugging Face\n",
    "2. Pulling a model from the Hugging Face Hub\n",
    "3. Loading and using the model for inference\n",
    "4. Fine-tuning a model for your specific needs (basic example)\n",
    "5. Saving and sharing your model\n",
    "\n",
    "Let's start with the basics and explore the Hugging Face ecosystem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Your Environment\n",
    "\n",
    "First, we need to install the necessary packages. The main package we'll use is `transformers`, which is Hugging Face's primary library for working with pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install transformers datasets torch accelerate evaluate\n",
    "\n",
    "# Check the version to make sure installation was successful\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Hugging Face Authentication (Optional)\n",
    "\n",
    "If you want to access gated models or upload your own models, you'll need to authenticate with Hugging Face. This step is optional for public models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Run this cell and enter your token when prompted\n",
    "# You can get your token from https://huggingface.co/settings/tokens\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pulling a Model from Hugging Face Hub\n",
    "\n",
    "Hugging Face offers thousands of models across various tasks. Let's start with a simple text classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# This is a sentiment analysis model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model size: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Model and Tokenizer\n",
    "\n",
    "- **Model**: The neural network architecture loaded with pre-trained weights\n",
    "- **Tokenizer**: Converts raw text into tokens that the model can understand\n",
    "\n",
    "Let's explore what the model can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model's configuration\n",
    "print(\"Model configuration:\")\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using the Model for Inference\n",
    "\n",
    "Now let's use our model to analyze some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to get sentiment predictions\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Get predictions from the model\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Convert to probabilities with softmax\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Get the most likely class\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    # Map class index to label\n",
    "    sentiment = \"positive\" if predicted_class == 1 else \"negative\"\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return sentiment, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with some example texts\n",
    "examples = [\n",
    "    \"I love this product! It's amazing and works perfectly.\",\n",
    "    \"The service was terrible and the staff was rude.\",\n",
    "    \"The movie was okay, not great but not terrible either.\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    sentiment, confidence = analyze_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment} (Confidence: {confidence:.4f})\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a Different Model: Text Generation\n",
    "\n",
    "Let's also try a text generation model to see how easy it is to switch between different types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Using a small GPT-2 model for text generation\n",
    "gen_model_name = \"distilgpt2\"\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model=gen_model_name)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"In a world where AI and humans work together, \"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning a Model for Your Specific Task\n",
    "\n",
    "One of the most powerful features of Hugging Face is the ability to fine-tune pre-trained models on your own data. Let's see how to fine-tune a text classification model on a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load a small dataset for demonstration\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine some examples from the dataset\n",
    "for i in range(3):\n",
    "    print(f\"Text: {dataset['train'][i]['text']}\")\n",
    "    print(f\"Label: {dataset['train'][i]['label']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for fine-tuning\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Load a small model for fine-tuning\n",
    "model_name_ft = \"distilbert-base-uncased\"\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(model_name_ft)\n",
    "model_ft = AutoModelForSequenceClassification.from_pretrained(model_name_ft, num_labels=3)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare for training - use a small subset for demonstration\n",
    "small_train_dataset = tokenized_datasets[\"train\"].select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"].select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,  # Set to True if you want to upload to Hugging Face Hub\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model_ft,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model (this will take a few minutes)\n",
    "# If you're running this in a notebook with limited resources,\n",
    "# you might want to skip this step or use even smaller datasets\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving and Sharing Your Model\n",
    "\n",
    "After fine-tuning, you can save your model locally or push it to Hugging Face Hub to share with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer locally\n",
    "model_ft.save_pretrained(\"./my_finetuned_model\")\n",
    "tokenizer_ft.save_pretrained(\"./my_finetuned_model\")\n",
    "\n",
    "print(\"Model saved to ./my_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to push to the Hub (requires login)\n",
    "# Uncomment and run these lines\n",
    "\"\"\"\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# Define your model name (must be unique on your account)\n",
    "model_name = \"your-username/tweet-sentiment-model\"\n",
    "\n",
    "# Push the model to the Hub\n",
    "if HfFolder.get_token() is not None:  # Check if logged in\n",
    "    trainer.push_to_hub(model_name, private=True)  # Set private=False to make it public\n",
    "    print(f\"Model pushed to Hugging Face Hub: {model_name}\")\n",
    "else:\n",
    "    print(\"You need to login first using login() function\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loading and Using Your Fine-tuned Model\n",
    "\n",
    "Once you've saved your model, you can load it just like any other Hugging Face model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model from disk\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./my_finetuned_model\")\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"./my_finetuned_model\")\n",
    "\n",
    "# Create a function to use the fine-tuned model\n",
    "def analyze_tweet_sentiment(text):\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "    \n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    # For tweet_eval sentiment: 0 = negative, 1 = neutral, 2 = positive\n",
    "    sentiment_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    sentiment = sentiment_map[predicted_class]\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return sentiment, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the fine-tuned model with some examples\n",
    "tweet_examples = [\n",
    "    \"Just had the best coffee ever! #happy\",\n",
    "    \"Waiting in line at the DMV. So bored.\",\n",
    "    \"Weather is cloudy today but I'm okay with it.\"\n",
    "]\n",
    "\n",
    "for text in tweet_examples:\n",
    "    sentiment, confidence = analyze_tweet_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment} (Confidence: {confidence:.4f})\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Experimenting with Other Model Types\n",
    "\n",
    "Hugging Face offers many different types of models for various tasks. Let's explore a few more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a named entity recognition pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Try it out\n",
    "text = \"My name is Sarah and I work at Google in New York City.\"\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "print(\"Named Entities:\")\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']}: {entity['entity']} (Score: {entity['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Download an example image\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Display the image\n",
    "image.show()  # This works in Jupyter notebook\n",
    "\n",
    "# Load image classification model and feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Preprocess the image and get predictions\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploying a Model for Production\n",
    "\n",
    "In a production environment, you'll want to serve your model efficiently. There are several ways to deploy Hugging Face models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Simple Flask API\n",
    "\n",
    "Here's a basic example of creating a Flask API for your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this to a file called app.py\n",
    "\"\"\"\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model (do this outside of route to load only once)\n",
    "model_path = \"./my_finetuned_model\"  # Or use a model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    text = data['text']\n",
    "    \n",
    "    # Tokenize and predict\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    # For tweet_eval sentiment: 0 = negative, 1 = neutral, 2 = positive\n",
    "    sentiment_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    sentiment = sentiment_map[predicted_class]\n",
    "    \n",
    "    return jsonify({\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': confidence\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "\"\"\"\n",
    "\n",
    "# To run this:\n",
    "# python app.py\n",
    "# Then, make POST requests to http://localhost:5000/predict with JSON data: {\"text\": \"your text here\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Using Hugging Face Inference API\n",
    "\n",
    "If you push your model to Hugging Face Hub, you can use their Inference API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to use Hugging Face Inference API\n",
    "\"\"\"\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/your-username/your-model-name\"\n",
    "headers = {\"Authorization\": f\"Bearer {YOUR_API_TOKEN}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "    \n",
    "output = query({\"inputs\": \"The food was delicious and the service excellent.\"})\n",
    "print(output)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices and Tips\n",
    "\n",
    "Here are some best practices when working with Hugging Face models:\n",
    "\n",
    "1. **Model Selection**: Choose the smallest model that meets your needs to minimize resource usage and latency.\n",
    "\n",
    "2. **Quantization**: For deployment, consider quantizing your model to reduce its size and increase inference speed.\n",
    "   ```python\n",
    "   # Example of quantizing a model\n",
    "   from transformers import AutoModelForSequenceClassification\n",
    "   import torch\n",
    "   \n",
    "   model = AutoModelForSequenceClassification.from_pretrained(\"your-model-name\")\n",
    "   quantized_model = torch.quantization.quantize_dynamic(\n",
    "       model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Batching**: Process multiple inputs in a batch to improve throughput.\n",
    "\n",
    "4. **Caching**: Cache tokenization results and model outputs for repeated requests.\n",
    "\n",
    "5. **Hardware Acceleration**: Use GPU or TPU when available for faster inference and training.\n",
    "\n",
    "6. **Fine-tuning Strategies**: Consider techniques like parameter-efficient fine-tuning (e.g., LoRA, adapters) to reduce computational costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exploring the Hugging Face Hub\n",
    "\n",
    "The Hugging Face Hub contains thousands of models for different tasks. Here's how to search for models programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Search for models by task\n",
    "models = api.list_models(filter=\"text-classification\", limit=5)\n",
    "\n",
    "print(\"Some text classification models on the Hub:\")\n",
    "for model in models:\n",
    "    print(f\"- {model.id} (Downloads: {model.downloads})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've covered how to:\n",
    "- Install and set up Hugging Face libraries\n",
    "- Load pre-trained models from the Hub\n",
    "- Use models for text classification, generation, and other tasks\n",
    "- Fine-tune a model on a custom dataset\n",
    "- Save and share your models\n",
    "- Deploy models for production use\n",
    "\n",
    "Hugging Face provides a powerful ecosystem that makes working with state-of-the-art models accessible to everyone. By combining pre-trained models with your own data, you can quickly develop powerful AI solutions for a wide range of tasks.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To continue learning, you can:\n",
    "1. Explore more model types on the [Hugging Face Hub](https://huggingface.co/models)\n",
    "2. Read the [Transformers documentation](https://huggingface.co/docs/transformers/index)\n",
    "3. Try fine-tuning on your own datasets\n",
    "4. Experiment with advanced techniques like parameter-efficient fine-tuning\n",
    "\n",
    "Happy modeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
